# ç¬¬11ç« ï¼šå¹¶å‘å’Œå¼‚æ­¥ - æ€§èƒ½ä¼˜åŒ–

> **æœ¬ç« ç›®æ ‡**ï¼šç†è§£å¹¶å‘ç¼–ç¨‹ï¼Œå­¦ä¼šä½¿ç”¨å¤šçº¿ç¨‹ã€åç¨‹æå‡æ€§èƒ½

---

## ğŸ“‹ æœ¬ç« å¤§çº²

1. [å¹¶å‘ vs å¹¶è¡Œ](#1-å¹¶å‘-vs-å¹¶è¡Œ)
2. [Python å¹¶å‘ç¼–ç¨‹](#2-python-å¹¶å‘ç¼–ç¨‹)
3. [Go å¹¶å‘ç¼–ç¨‹](#3-go-å¹¶å‘ç¼–ç¨‹)
4. [å¸¸è§å¹¶å‘é—®é¢˜](#4-å¸¸è§å¹¶å‘é—®é¢˜)
5. [å¼‚æ­¥ç¼–ç¨‹](#5-å¼‚æ­¥ç¼–ç¨‹)
6. [NOFX çš„å¹¶å‘å®è·µ](#6-nofx-çš„å¹¶å‘å®è·µ)
7. [å®æˆ˜ç»ƒä¹ ](#7-å®æˆ˜ç»ƒä¹ )

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š4-5 å°æ—¶

---

## 1. å¹¶å‘ vs å¹¶è¡Œ

### 1.1 æ¦‚å¿µåŒºåˆ†

```
å¹¶å‘ï¼ˆConcurrencyï¼‰ï¼šå¤šä¸ªä»»åŠ¡äº¤æ›¿æ‰§è¡Œ
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚ä»»åŠ¡Aâ”‚ â”‚ä»»åŠ¡Bâ”‚ â”‚ä»»åŠ¡Aâ”‚  â† å•æ ¸CPUå¿«é€Ÿåˆ‡æ¢
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
    æ—¶é—´ â”€â”€â”€â”€â”€â”€â”€â”€â†’

å¹¶è¡Œï¼ˆParallelismï¼‰ï¼šå¤šä¸ªä»»åŠ¡åŒæ—¶æ‰§è¡Œ
â”Œâ”€â”€â”€â”€â”€â”
â”‚ä»»åŠ¡Aâ”‚  â† CPUæ ¸å¿ƒ1
â””â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”
â”‚ä»»åŠ¡Bâ”‚  â† CPUæ ¸å¿ƒ2
â””â”€â”€â”€â”€â”€â”˜
    æ—¶é—´ â”€â”€â”€â”€â”€â”€â”€â”€â†’
```

### 1.2 ç”Ÿæ´»åŒ–ç±»æ¯”

**å¹¶å‘**ï¼šä¸€ä¸ªå¨å¸ˆåŒæ—¶åšå¤šé“èœ
- ç‚’èœA â†’ ç­‰å¾…ç…®æ²¸ â†’ åˆ‡èœB â†’ å›å»ç¿»ç‚’A â†’ å¤„ç†èœC
- çœ‹èµ·æ¥åœ¨"åŒæ—¶"åšï¼Œå®é™…æ˜¯å¿«é€Ÿåˆ‡æ¢

**å¹¶è¡Œ**ï¼šå¤šä¸ªå¨å¸ˆå„è‡ªåšèœ
- å¨å¸ˆ1åšèœA
- å¨å¸ˆ2åšèœB
- å¨å¸ˆ3åšèœC
- çœŸæ­£çš„åŒæ—¶è¿›è¡Œ

### 1.3 ä»€ä¹ˆæ—¶å€™éœ€è¦å¹¶å‘

**é€‚åˆå¹¶å‘çš„åœºæ™¯**ï¼š
- âœ… **I/O å¯†é›†å‹**ï¼šç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ã€æ•°æ®åº“æŸ¥è¯¢
- âœ… **ç­‰å¾…æ—¶é—´é•¿**ï¼šç­‰å¾…APIå“åº”ã€ç­‰å¾…ç”¨æˆ·è¾“å…¥
- âŒ **CPU å¯†é›†å‹**ï¼šå¤æ‚è®¡ç®—ã€å›¾åƒå¤„ç†ï¼ˆéœ€è¦å¹¶è¡Œï¼‰

**ä¾‹å­**ï¼š
```python
# ä¸²è¡Œæ‰§è¡Œï¼ˆæ…¢ï¼‰
def download_files(urls):
    results = []
    for url in urls:
        data = download(url)  # æ¯ä¸ªä¸‹è½½éœ€è¦2ç§’
        results.append(data)
    return results

# ä¸‹è½½100ä¸ªæ–‡ä»¶éœ€è¦ 100 * 2 = 200ç§’

# å¹¶å‘æ‰§è¡Œï¼ˆå¿«ï¼‰
def download_files_concurrent(urls):
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(download, urls)
    return list(results)

# ä¸‹è½½100ä¸ªæ–‡ä»¶åªéœ€ 100 / 10 * 2 = 20ç§’
```

---

## 2. Python å¹¶å‘ç¼–ç¨‹

### 2.1 å¤šçº¿ç¨‹ï¼ˆThreadingï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šI/O å¯†é›†å‹ä»»åŠ¡

```python
import threading
import time
import requests

# ç¤ºä¾‹ï¼šå¹¶å‘ä¸‹è½½å¤šä¸ªç½‘é¡µ
def download_page(url):
    print(f"[{threading.current_thread().name}] å¼€å§‹ä¸‹è½½ {url}")
    response = requests.get(url)
    print(f"[{threading.current_thread().name}] ä¸‹è½½å®Œæˆ {url}ï¼Œå¤§å° {len(response.content)} å­—èŠ‚")
    return response.content

# æ–¹æ³•1ï¼šæ‰‹åŠ¨åˆ›å»ºçº¿ç¨‹
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3",
]

threads = []
for url in urls:
    thread = threading.Thread(target=download_page, args=(url,))
    threads.append(thread)
    thread.start()

# ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
for thread in threads:
    thread.join()

print("æ‰€æœ‰ä¸‹è½½å®Œæˆ")
```

**æ–¹æ³•2ï¼šä½¿ç”¨çº¿ç¨‹æ± **ï¼ˆæ¨èï¼‰

```python
from concurrent.futures import ThreadPoolExecutor

urls = ["https://example.com/page1", "https://example.com/page2", "https://example.com/page3"]

# åˆ›å»ºçº¿ç¨‹æ± ï¼Œæœ€å¤š5ä¸ªå¹¶å‘çº¿ç¨‹
with ThreadPoolExecutor(max_workers=5) as executor:
    # æäº¤ä»»åŠ¡
    futures = [executor.submit(download_page, url) for url in urls]

    # è·å–ç»“æœ
    for future in futures:
        result = future.result()  # é˜»å¡ç­‰å¾…ç»“æœ
        print(f"è·å¾—ç»“æœï¼Œå¤§å° {len(result)} å­—èŠ‚")
```

**ä½¿ç”¨ mapï¼ˆæ›´ç®€æ´ï¼‰**ï¼š

```python
with ThreadPoolExecutor(max_workers=5) as executor:
    results = executor.map(download_page, urls)
    for result in results:
        print(f"ä¸‹è½½å®Œæˆï¼Œå¤§å° {len(result)} å­—èŠ‚")
```

### 2.2 å¤šè¿›ç¨‹ï¼ˆMultiprocessingï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šCPU å¯†é›†å‹ä»»åŠ¡

```python
from multiprocessing import Pool
import time

# CPUå¯†é›†å‹ä»»åŠ¡ï¼šè®¡ç®—å¤§æ•°çš„å¹³æ–¹
def calculate_square(n):
    print(f"è®¡ç®— {n} çš„å¹³æ–¹")
    result = 0
    for i in range(n):
        result += i * i
    return result

if __name__ == "__main__":
    numbers = [10000000, 20000000, 30000000, 40000000]

    # ä¸²è¡Œæ‰§è¡Œ
    start = time.time()
    results = [calculate_square(n) for n in numbers]
    print(f"ä¸²è¡Œè€—æ—¶: {time.time() - start:.2f}ç§’")

    # å¹¶è¡Œæ‰§è¡Œ
    start = time.time()
    with Pool(processes=4) as pool:
        results = pool.map(calculate_square, numbers)
    print(f"å¹¶è¡Œè€—æ—¶: {time.time() - start:.2f}ç§’")
```

**å¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹**ï¼š

| | å¤šçº¿ç¨‹ | å¤šè¿›ç¨‹ |
|---|---|---|
| **é€‚ç”¨åœºæ™¯** | I/O å¯†é›†å‹ | CPU å¯†é›†å‹ |
| **å†…å­˜** | å…±äº«å†…å­˜ | ç‹¬ç«‹å†…å­˜ |
| **å¼€é”€** | å° | å¤§ |
| **GILé™åˆ¶** | å—é™ï¼ˆPythonï¼‰ | ä¸å—é™ |
| **é€šä¿¡** | ç®€å• | å¤æ‚ |

### 2.3 asyncioï¼ˆå¼‚æ­¥åç¨‹ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šé«˜å¹¶å‘ I/Oï¼ˆå¦‚ Web æœåŠ¡å™¨ï¼‰

```python
import asyncio
import aiohttp
import time

# å¼‚æ­¥ä¸‹è½½
async def download_page_async(session, url):
    print(f"å¼€å§‹ä¸‹è½½ {url}")
    async with session.get(url) as response:
        content = await response.read()
        print(f"ä¸‹è½½å®Œæˆ {url}ï¼Œå¤§å° {len(content)} å­—èŠ‚")
        return content

async def main():
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3",
    ]

    async with aiohttp.ClientSession() as session:
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [download_page_async(session, url) for url in urls]
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks)

    print(f"ä¸‹è½½ {len(results)} ä¸ªé¡µé¢")

# è¿è¡Œ
if __name__ == "__main__":
    start = time.time()
    asyncio.run(main())
    print(f"æ€»è€—æ—¶: {time.time() - start:.2f}ç§’")
```

**å¯¹æ¯”ä¸‰ç§æ–¹å¼**ï¼š

```python
# 1. ä¸²è¡Œï¼ˆæœ€æ…¢ï¼‰
def download_serial(urls):
    return [download(url) for url in urls]

# 2. å¤šçº¿ç¨‹ï¼ˆé€‚åˆI/Oï¼Œä¸­ç­‰é€Ÿåº¦ï¼‰
def download_threading(urls):
    with ThreadPoolExecutor(max_workers=10) as executor:
        return list(executor.map(download, urls))

# 3. asyncioï¼ˆé€‚åˆé«˜å¹¶å‘I/Oï¼Œæœ€å¿«ï¼‰
async def download_async(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [download_page_async(session, url) for url in urls]
        return await asyncio.gather(*tasks)
```

---

## 3. Go å¹¶å‘ç¼–ç¨‹

### 3.1 Goroutine

**Go çš„å¹¶å‘æ¨¡å‹**ï¼šè½»é‡çº§åç¨‹

```go
package main

import (
    "fmt"
    "time"
)

func downloadPage(url string) {
    fmt.Printf("å¼€å§‹ä¸‹è½½ %s\n", url)
    time.Sleep(2 * time.Second) // æ¨¡æ‹Ÿä¸‹è½½
    fmt.Printf("ä¸‹è½½å®Œæˆ %s\n", url)
}

func main() {
    urls := []string{
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3",
    }

    // å¯åŠ¨å¹¶å‘ goroutine
    for _, url := range urls {
        go downloadPage(url)  // go å…³é”®å­—å¯åŠ¨åç¨‹
    }

    // ç­‰å¾…æ‰€æœ‰ goroutine å®Œæˆ
    time.Sleep(3 * time.Second)
    fmt.Println("æ‰€æœ‰ä¸‹è½½å®Œæˆ")
}
```

### 3.2 Channelï¼ˆé€šé“ï¼‰

**ç”¨äº goroutine é—´é€šä¿¡**ï¼š

```go
package main

import "fmt"

func worker(id int, jobs <-chan int, results chan<- int) {
    for job := range jobs {
        fmt.Printf("Worker %d å¤„ç†ä»»åŠ¡ %d\n", id, job)
        results <- job * 2  // å‘é€ç»“æœåˆ° channel
    }
}

func main() {
    jobs := make(chan int, 100)
    results := make(chan int, 100)

    // å¯åŠ¨3ä¸ª worker
    for w := 1; w <= 3; w++ {
        go worker(w, jobs, results)
    }

    // å‘é€9ä¸ªä»»åŠ¡
    for j := 1; j <= 9; j++ {
        jobs <- j
    }
    close(jobs)

    // æ”¶é›†ç»“æœ
    for a := 1; a <= 9; a++ {
        result := <-results
        fmt.Printf("ç»“æœ: %d\n", result)
    }
}
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Worker 1 å¤„ç†ä»»åŠ¡ 1
Worker 2 å¤„ç†ä»»åŠ¡ 2
Worker 3 å¤„ç†ä»»åŠ¡ 3
Worker 1 å¤„ç†ä»»åŠ¡ 4
...
ç»“æœ: 2
ç»“æœ: 4
ç»“æœ: 6
...
```

### 3.3 WaitGroup

**ç­‰å¾…å¤šä¸ª goroutine å®Œæˆ**ï¼š

```go
package main

import (
    "fmt"
    "sync"
    "time"
)

func task(id int, wg *sync.WaitGroup) {
    defer wg.Done()  // å®Œæˆæ—¶é€šçŸ¥

    fmt.Printf("ä»»åŠ¡ %d å¼€å§‹\n", id)
    time.Sleep(time.Second)
    fmt.Printf("ä»»åŠ¡ %d å®Œæˆ\n", id)
}

func main() {
    var wg sync.WaitGroup

    for i := 1; i <= 5; i++ {
        wg.Add(1)  // å¢åŠ ç­‰å¾…è®¡æ•°
        go task(i, &wg)
    }

    wg.Wait()  // é˜»å¡ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    fmt.Println("æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
}
```

### 3.4 å®æˆ˜ï¼šå¹¶å‘æŠ“å– API æ•°æ®

```go
package main

import (
    "encoding/json"
    "fmt"
    "io/ioutil"
    "net/http"
    "sync"
)

type Result struct {
    URL    string
    Status int
    Error  error
}

func fetchURL(url string, results chan<- Result, wg *sync.WaitGroup) {
    defer wg.Done()

    resp, err := http.Get(url)
    if err != nil {
        results <- Result{URL: url, Error: err}
        return
    }
    defer resp.Body.Close()

    results <- Result{URL: url, Status: resp.StatusCode}
}

func main() {
    urls := []string{
        "https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT",
        "https://api.binance.com/api/v3/ticker/price?symbol=ETHUSDT",
        "https://api.binance.com/api/v3/ticker/price?symbol=BNBUSDT",
    }

    results := make(chan Result, len(urls))
    var wg sync.WaitGroup

    // å¯åŠ¨å¹¶å‘è¯·æ±‚
    for _, url := range urls {
        wg.Add(1)
        go fetchURL(url, results, &wg)
    }

    // å…³é—­ç»“æœé€šé“ï¼ˆåœ¨æ‰€æœ‰ä»»åŠ¡å®Œæˆåï¼‰
    go func() {
        wg.Wait()
        close(results)
    }()

    // æ”¶é›†ç»“æœ
    for result := range results {
        if result.Error != nil {
            fmt.Printf("âŒ %s å¤±è´¥: %v\n", result.URL, result.Error)
        } else {
            fmt.Printf("âœ… %s æˆåŠŸ (çŠ¶æ€ç : %d)\n", result.URL, result.Status)
        }
    }
}
```

---

## 4. å¸¸è§å¹¶å‘é—®é¢˜

### 4.1 ç«æ€æ¡ä»¶ï¼ˆRace Conditionï¼‰

**é—®é¢˜**ï¼šå¤šä¸ªçº¿ç¨‹åŒæ—¶ä¿®æ”¹åŒä¸€å˜é‡

```python
import threading

counter = 0

def increment():
    global counter
    for _ in range(100000):
        counter += 1  # ä¸æ˜¯åŸå­æ“ä½œï¼

# å¯åŠ¨ä¸¤ä¸ªçº¿ç¨‹
t1 = threading.Thread(target=increment)
t2 = threading.Thread(target=increment)
t1.start()
t2.start()
t1.join()
t2.join()

print(f"Counter: {counter}")  # æœŸæœ› 200000ï¼Œå®é™…å¯èƒ½å°äº
```

**åŸå› **ï¼š`counter += 1` åˆ†ä¸º3æ­¥
1. è¯»å– counter
2. åŠ  1
3. å†™å› counter

ä¸¤ä¸ªçº¿ç¨‹å¯èƒ½åŒæ—¶æ‰§è¡Œï¼Œå¯¼è‡´ä¸¢å¤±æ›´æ–°ã€‚

### 4.2 ä½¿ç”¨é”ï¼ˆLockï¼‰

**Python è§£å†³æ–¹æ¡ˆ**ï¼š

```python
import threading

counter = 0
lock = threading.Lock()

def increment_safe():
    global counter
    for _ in range(100000):
        with lock:  # è·å–é”
            counter += 1  # ä¸´ç•ŒåŒºï¼ŒåŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œ

t1 = threading.Thread(target=increment_safe)
t2 = threading.Thread(target=increment_safe)
t1.start()
t2.start()
t1.join()
t2.join()

print(f"Counter: {counter}")  # æ­£ç¡®è¾“å‡º 200000
```

**Go è§£å†³æ–¹æ¡ˆ**ï¼š

```go
package main

import (
    "fmt"
    "sync"
)

func main() {
    var counter int
    var mu sync.Mutex
    var wg sync.WaitGroup

    increment := func() {
        defer wg.Done()
        for i := 0; i < 100000; i++ {
            mu.Lock()
            counter++
            mu.Unlock()
        }
    }

    wg.Add(2)
    go increment()
    go increment()
    wg.Wait()

    fmt.Printf("Counter: %d\n", counter)  // æ­£ç¡®è¾“å‡º 200000
}
```

### 4.3 æ­»é”ï¼ˆDeadlockï¼‰

**é—®é¢˜**ï¼šä¸¤ä¸ªçº¿ç¨‹äº’ç›¸ç­‰å¾…å¯¹æ–¹é‡Šæ”¾é”

```python
import threading

lock1 = threading.Lock()
lock2 = threading.Lock()

def task1():
    with lock1:
        print("ä»»åŠ¡1è·å¾—é”1")
        time.sleep(0.1)
        with lock2:  # ç­‰å¾…é”2
            print("ä»»åŠ¡1è·å¾—é”2")

def task2():
    with lock2:
        print("ä»»åŠ¡2è·å¾—é”2")
        time.sleep(0.1)
        with lock1:  # ç­‰å¾…é”1
            print("ä»»åŠ¡2è·å¾—é”1")

# å¯èƒ½æ­»é”ï¼
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **å›ºå®šè·å–é”çš„é¡ºåº**ï¼ˆæ€»æ˜¯å…ˆè·å– lock1ï¼Œå†è·å– lock2ï¼‰
2. **ä½¿ç”¨è¶…æ—¶**ï¼ˆ`lock.acquire(timeout=1)`ï¼‰
3. **é¿å…åµŒå¥—é”**

---

## 5. å¼‚æ­¥ç¼–ç¨‹

### 5.1 ä»€ä¹ˆæ˜¯å¼‚æ­¥

**åŒæ­¥ vs å¼‚æ­¥**ï¼š

```python
# åŒæ­¥ï¼ˆé˜»å¡ï¼‰
def get_data():
    data = database.query()  # ç­‰å¾…æ•°æ®åº“è¿”å›ï¼ˆé˜»å¡ï¼‰
    return data

# å¼‚æ­¥ï¼ˆéé˜»å¡ï¼‰
async def get_data_async():
    data = await database.query()  # ç­‰å¾…æ—¶å¯ä»¥æ‰§è¡Œå…¶ä»–ä»»åŠ¡
    return data
```

**ç±»æ¯”**ï¼š
- **åŒæ­¥**ï¼šå»é¤å…ç‚¹é¤ï¼Œç«™åœ¨æŸœå°ç­‰é¤ï¼ˆé˜»å¡ï¼‰
- **å¼‚æ­¥**ï¼šå»é¤å…ç‚¹é¤ï¼Œæ‹¿å·ç‰Œå›åº§ä½ç­‰å«å·ï¼ˆéé˜»å¡ï¼‰

### 5.2 Python asyncio è¯¦è§£

**åŸºç¡€ç¤ºä¾‹**ï¼š

```python
import asyncio

async def fetch_data(delay, id):
    print(f"[{id}] å¼€å§‹è·å–æ•°æ®")
    await asyncio.sleep(delay)  # æ¨¡æ‹Ÿå¼‚æ­¥I/O
    print(f"[{id}] æ•°æ®è·å–å®Œæˆ")
    return f"æ•°æ®{id}"

async def main():
    # å¹¶å‘æ‰§è¡Œ3ä¸ªä»»åŠ¡
    results = await asyncio.gather(
        fetch_data(2, 1),
        fetch_data(1, 2),
        fetch_data(3, 3),
    )
    print(f"æ‰€æœ‰ç»“æœ: {results}")

asyncio.run(main())
```

**è¾“å‡º**ï¼š
```
[1] å¼€å§‹è·å–æ•°æ®
[2] å¼€å§‹è·å–æ•°æ®
[3] å¼€å§‹è·å–æ•°æ®
[2] æ•°æ®è·å–å®Œæˆ  â† 1ç§’å
[1] æ•°æ®è·å–å®Œæˆ  â† 2ç§’å
[3] æ•°æ®è·å–å®Œæˆ  â† 3ç§’å
æ‰€æœ‰ç»“æœ: ['æ•°æ®1', 'æ•°æ®2', 'æ•°æ®3']
```

**å®æˆ˜ï¼šå¼‚æ­¥APIè°ƒç”¨**ï¼š

```python
import asyncio
import aiohttp

async def fetch_price(session, symbol):
    url = f"https://api.binance.com/api/v3/ticker/price?symbol={symbol}"
    async with session.get(url) as response:
        data = await response.json()
        return {symbol: data['price']}

async def get_all_prices():
    symbols = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'DOGEUSDT']

    async with aiohttp.ClientSession() as session:
        tasks = [fetch_price(session, symbol) for symbol in symbols]
        results = await asyncio.gather(*tasks)

    # åˆå¹¶ç»“æœ
    prices = {}
    for result in results:
        prices.update(result)

    return prices

# è¿è¡Œ
prices = asyncio.run(get_all_prices())
print(prices)
# {'BTCUSDT': '45000.00', 'ETHUSDT': '3000.00', ...}
```

---

## 6. NOFX çš„å¹¶å‘å®è·µ

### 6.1 Goroutine å¯åŠ¨ Trader

**æ–‡ä»¶**ï¼š`cmd/main.go:85`

```go
// ä¸ºæ¯ä¸ªäº¤æ˜“å‘˜å¯åŠ¨ goroutine
for _, trader := range traders {
    wg.Add(1)
    go func(t interfaces.Trader) {
        defer wg.Done()

        if err := t.Start(); err != nil {
            log.Printf("å¯åŠ¨äº¤æ˜“å‘˜å¤±è´¥: %v", err)
        }
    }(trader)
}

// ç­‰å¾…æ‰€æœ‰äº¤æ˜“å‘˜å®Œæˆ
wg.Wait()
```

**ä½œç”¨**ï¼šå¤šä¸ªäº¤æ˜“å‘˜å¯ä»¥å¹¶å‘è¿è¡Œï¼Œäº’ä¸é˜»å¡ã€‚

### 6.2 å®šæ—¶å™¨å¹¶å‘

**æ–‡ä»¶**ï¼š`trader/binance_futures.go:172`

```go
func (t *BinanceFuturesTrader) Start() error {
    ticker := time.NewTicker(t.config.UpdateInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            // å®šæ—¶æ‰§è¡Œç­–ç•¥
            if err := t.executeStrategy(); err != nil {
                log.Printf("æ‰§è¡Œç­–ç•¥å¤±è´¥: %v", err)
            }
        case <-t.stopChan:
            return nil
        }
    }
}
```

**è¯´æ˜**ï¼š
- `time.NewTicker`ï¼šåˆ›å»ºå®šæ—¶å™¨
- `select`ï¼šç­‰å¾…å¤šä¸ª channel äº‹ä»¶
- æ¯éš”ä¸€æ®µæ—¶é—´æ‰§è¡Œä¸€æ¬¡ç­–ç•¥åˆ¤æ–­

### 6.3 API å¹¶å‘æŸ¥è¯¢

**æ–‡ä»¶**ï¼š`api/server.go:150`

```go
func (s *Server) handleAllPositions(c *gin.Context) {
    var allPositions []interface{}
    var mu sync.Mutex
    var wg sync.WaitGroup

    // å¹¶å‘è·å–æ‰€æœ‰äº¤æ˜“å‘˜çš„æŒä»“
    for _, trader := range s.traderManager.GetAllTraders() {
        wg.Add(1)
        go func(t interfaces.Trader) {
            defer wg.Done()

            positions, _ := t.GetPositions()

            mu.Lock()
            allPositions = append(allPositions, positions...)
            mu.Unlock()
        }(trader)
    }

    wg.Wait()
    c.JSON(200, allPositions)
}
```

**ä¼˜åŠ¿**ï¼šå¤šä¸ªäº¤æ˜“å‘˜çš„æŒä»“æŸ¥è¯¢å¹¶å‘æ‰§è¡Œï¼Œå“åº”æ›´å¿«ã€‚

---

## 7. å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹  1ï¼šå¹¶å‘ä¸‹è½½å›¾ç‰‡

ç¼–å†™ç¨‹åºï¼Œå¹¶å‘ä¸‹è½½10å¼ å›¾ç‰‡ã€‚

**è¦æ±‚**ï¼š
- ä½¿ç”¨ `ThreadPoolExecutor`
- æ˜¾ç¤ºä¸‹è½½è¿›åº¦
- ä¿å­˜åˆ°æœ¬åœ°

**æç¤º**ï¼š
```python
from concurrent.futures import ThreadPoolExecutor
import requests

def download_image(url, filename):
    # å®ç°ä¸‹è½½é€»è¾‘
    pass

urls = [
    "https://example.com/image1.jpg",
    "https://example.com/image2.jpg",
    # ...
]

with ThreadPoolExecutor(max_workers=5) as executor:
    # æäº¤ä»»åŠ¡
    pass
```

### ç»ƒä¹  2ï¼šå¼‚æ­¥çˆ¬è™«

ä½¿ç”¨ `asyncio` å’Œ `aiohttp` æŠ“å–å¤šä¸ªæ–°é—»ç½‘ç«™çš„æ ‡é¢˜ã€‚

**è¦æ±‚**ï¼š
- è‡³å°‘5ä¸ªURL
- æå–æ ‡é¢˜ï¼ˆä½¿ç”¨ BeautifulSoupï¼‰
- æµ‹é‡æ€»è€—æ—¶

### ç»ƒä¹  3ï¼šGo Worker Pool

å®ç°ä¸€ä¸ª Worker Poolï¼Œå¤„ç†100ä¸ªä»»åŠ¡ã€‚

**è¦æ±‚**ï¼š
- 5ä¸ª worker goroutine
- ä½¿ç”¨ channel ä¼ é€’ä»»åŠ¡
- ç»Ÿè®¡æ¯ä¸ª worker å¤„ç†çš„ä»»åŠ¡æ•°

---

## æœ¬ç« æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µ

1. **å¹¶å‘ â‰  å¹¶è¡Œ**
   - å¹¶å‘ï¼šäº¤æ›¿æ‰§è¡Œ
   - å¹¶è¡Œï¼šåŒæ—¶æ‰§è¡Œ

2. **é€‰æ‹©åˆé€‚çš„å·¥å…·**
   - I/Oå¯†é›†å‹ â†’ å¤šçº¿ç¨‹ / asyncio
   - CPUå¯†é›†å‹ â†’ å¤šè¿›ç¨‹
   - é«˜å¹¶å‘I/O â†’ asyncio / goroutine

3. **æ³¨æ„å¹¶å‘é—®é¢˜**
   - ç«æ€æ¡ä»¶ â†’ ä½¿ç”¨é”
   - æ­»é” â†’ å›ºå®šé”é¡ºåº
   - èµ„æºæ³„æ¼ â†’ ä½¿ç”¨ `with` / `defer`

### æœ€ä½³å®è·µ

1. **ä¸è¦è¿‡åº¦å¹¶å‘**ï¼šå¹¶å‘ä¸æ˜¯è¶Šå¤šè¶Šå¥½
2. **åˆç†è®¾ç½®å¹¶å‘æ•°**ï¼š`ThreadPoolExecutor(max_workers=10)`
3. **ä½¿ç”¨é«˜çº§æŠ½è±¡**ï¼šä¼˜å…ˆä½¿ç”¨çº¿ç¨‹æ± ã€asyncioï¼Œè€Œéæ‰‹åŠ¨ç®¡ç†
4. **æµ‹è¯•å¹¶å‘ä»£ç **ï¼šå®¹æ˜“å‡ºç°éš¾ä»¥å¤ç°çš„bug

---

**ğŸ’¡ è®°ä½**ï¼šå¹¶å‘æ˜¯ä¸ºäº†æé«˜æ•ˆç‡ï¼Œè€Œä¸æ˜¯å¢åŠ å¤æ‚åº¦ã€‚å…ˆè®©ä»£ç æ­£ç¡®è¿è¡Œï¼Œå†è€ƒè™‘å¹¶å‘ä¼˜åŒ–ï¼
